{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # to cread the data csv\n",
    "import csv\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt \n",
    "#from PIL import Image # linked to generating a wordcloud\n",
    "#from os import path # for wordcloud\n",
    "#from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator # for wordcloud !! May only run with Python 3.10\n",
    "\n",
    "#import re #regular expressions\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import word_tokenize #turn strings into lists\n",
    "from nltk.corpus import stopwords # remove words like 'a', 'an' etc.\n",
    "stop_words = set(stopwords.words('english')) #imports the english stopwords\n",
    "#from nltk.stem import PorterStemmer #for 'stemming' words to improve search/assessments\n",
    "#stemmer = PorterStemmer()\n",
    "from nltk import TweetTokenizer #TweetTokenizer was tried so solve another problem \n",
    "# but for the context it may be more applicable than word_tokenize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to be called on a row to perform the tokenizing and cleaning functions\n",
    "\n",
    "def tokenize(dict_key):\n",
    "    data = dict_key.replace(\"[.?!,:;()\\-\\\"]\", \"\") # regex=True) what is the difference by not adding regex=True?\n",
    "    tokens = nltk.word_tokenize().tokenize(data)\n",
    "    tokens_no_stops = [word for word in tokens if word not in stop_words]\n",
    "    tokens_no_stops = [word for word in tokens_no_stops if word not in ['NaN', '...', 'im']]\n",
    "    #tokens_main += tokens_no_stops #this may be an issue - better try tokens_main.append(tokens_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_main = []\n",
    "with open('okcupid_profiles.csv', newline='') as file:\n",
    "    ok_csv = csv.DictReader(file)\n",
    "    for row in ok_csv:\n",
    "        #essay0_tokens = tokenize(row['essay0'])\n",
    "        #tokens_main.append(essay0_tokens)\n",
    "        #essay1_tokens = tokenize(row['essay1'])\n",
    "        #tokens_main.append(essay1_tokens)\n",
    "        #essay2_tokens = tokenize(row['essay2'])\n",
    "        #tokens_main.append(essay2_tokens)\n",
    "        #essay3_tokens = tokenize(row['essay3'])\n",
    "        #tokens_main.append(essay3_tokens)\n",
    "        #essay4_tokens = tokenize(row['essay4'])\n",
    "        #tokens_main.append(essay4_tokens)\n",
    "        #essay5_tokens = tokenize(row['essay5'])\n",
    "        #tokens_main.append(essay5_tokens)\n",
    "        #essay6_tokens = tokenize(row['essay6'])\n",
    "        #tokens_main.append(essay6_tokens)\n",
    "        #essay7_tokens = tokenize(row['essay7'])\n",
    "        #tokens_main.append(essay7_tokens)\n",
    "        #essay8_tokens = tokenize(row['essay8'])\n",
    "        #tokens_main.append(essay8_tokens)\n",
    "        #essay9_tokens = tokenize(row['essay9'])\n",
    "        #tokens_main.append(essay9_tokens)\n",
    "        #print(type(row['essay9']))\n",
    "        punc = str(row['essay9']).replace(\"[.?!,:;()\\-\\\"]\", \"\") #removed regex=True - what is the difference?\n",
    "        print(punc)\n",
    "        #tokens = tokens = nltk.TweetTokenizer().tokenize(str(punc))\n",
    "        #tokens_no_stops = [word for word in tokens if word not in stop_words]\n",
    "        #tokens_no_stops = [word for word in tokens_no_stops if word not in ['NaN', '...', 'im']]\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_main[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_dict = {}\n",
    "for word in tokens_main:\n",
    "    if word in tokens_dict:\n",
    "        tokens_dict[word] = tokens_dict[word]+1\n",
    "    else:\n",
    "        tokens_dict[word]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ok_csv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_counter(word, source):\n",
    "    for item in source:\n",
    "        if item == word:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "# to run:\n",
    "# caller = token_counter(word, source)\n",
    "# token_count = +1 if caller == True else token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(okcupid[][])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about me:  i would love to think that i was some some kind of intellectual: \n",
    "either the dumbest smart guy, or the smartest dumb guy. can'\n",
    "t say i can tell the difference. i love to talk about ideas and concepts.\n",
    "i forge odd metaphors instead of reciting cliches. like the simularities between a friend of mine's\n",
    "house and an underwater salt mine. my favorite word is salt by the way (weird choice i know). \n",
    "to me most things in life are better as metaphors. i seek to make myself a little better everyday, \n",
    "in some productively lazy way. got tired of tying my shoes. considered hiring \n",
    "a five year old, but would probably have to tie both of our shoes... decided to only wear leather shoes dress shoes.  \n",
    "about you:  you love to have really serious, really deep conversations about really silly stuff. you have to be willing to \n",
    "snap me out of a light hearted rant with a kiss. you don't have to be funny, but you have to be able to make me laugh. yo\n",
    "u should be able to bend spoons with your mind, and telepathically make me smile while i am still at work. you should love life, \n",
    "and be cool with just letting the wind blow. extra points for reading all this and guessing my favorite video game (no hints given yet). \n",
    "and lastly you have a good attention span.\n",
    "\n",
    "\n",
    "fave movie of all time: \"hook\" by steven spielberg.  \n",
    "the current reads: think and grow rich, the 21 irrefutable laws of leadership, life of mary: as seen by the mystics (weird mix)  \n",
    "fave shows: friends, lost, house, dancing with the stars, american idol  fave food: japanese food! mom's\n",
    "home cooking.\n",
    "\n",
    "\n",
    "im working on a degree in film at california state university san francisco. while \n",
    "working on that i actively seek out different partnerships for\n",
    " film projects. the main aspect i enjoy about film is the writing aspect, so i work on scripts in my down time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in okcupid.filter(regex=\"essay\\d+\", axis=1).columns:\n",
    "    print(okcupid[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = []\n",
    "for col in okcupid.filter(regex=\"essay\\d+\", axis=1).columns:\n",
    "    okcupid[col] = okcupid[col].str.replace(\"[.?!,:;()\\-\\\"]\", \"\", regex=True)\n",
    "    tokens = nltk.TweetTokenizer().tokenize(str(okcupid[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ok_csv('essay1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_main = []\n",
    "\n",
    "essay0 = tokenize(ok_csv[1]['essay0'])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer #TweetTokenizer was tried so solve another problem \n",
    "# but for the context it may be more applicable than word_tokenize()\n",
    "tokens_main = []\n",
    "for col in okcupid.filter(regex=\"essay\\d+\", axis=1).columns: #regex=\"essay\\d+\" tells to apply to cols with this title, \\d+ is a digit +1\n",
    "    #col_df = pd.DataFrame(col)\n",
    "    for index in col:\n",
    "        data = index.replace(\"[.?!,:;()\\-\\\"]\", \"\", regex=True)\n",
    "        tokens = nltk.TweetTokenizer().tokenize(str(data)) # I'd like to understand what okcupid[col] is withouth str() and why str() is needed\n",
    "        tokens_no_stops = [word for word in tokens if word not in stop_words]\n",
    "        tokens_no_stops = [word for word in tokens_no_stops if word not in ['NaN', '...', 'im']]\n",
    "        tokens_main += tokens_no_stops #this may be an issue - better try tokens_main.append(tokens_no_stops)\n",
    "        print(tokens)\n",
    "        print(f\"Column {col}: Number of tokens = {len(tokens)}, {len(tokens_no_stops)} words added after stopwords removed, total essay words now {len(tokens_main)}\")\n",
    "print(len(tokens_main))\n",
    "print(tokens_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "\n",
    "for token in tokens_main:\n",
    "    count = 0\n",
    "    for token2 in tokens_main:\n",
    "        if token == token2:\n",
    "            count += 1\n",
    "    word_freq[token] = count\n",
    "\n",
    "print(word_freq)\n",
    "print(word_freq['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = pd.DataFrame(tokens_main)\n",
    "tk.describe()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
